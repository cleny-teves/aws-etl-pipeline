# üöÄ AWS Serverless ETL Pipeline (IaC con CloudFormation)

Este repositorio contiene una soluci√≥n 100% Serverless para construir una canalizaci√≥n ETL totalmente automatizada sobre AWS.

Toda la infraestructura es desplegada utilizando **Infraestructura como C√≥digo (IaC)** con **AWS CloudFormation**, lo que permite reproducibilidad, escalabilidad y cero configuraci√≥n manual.

El objetivo es transformar datos crudos de ventas (JSON) en un formato optimizado **Parquet**, almacenarlos en un Data Lake y dejarlos listos para ser consultados con SQL mediante **Amazon Athena**.

## üèõÔ∏è Arquitectura de la Soluci√≥n

Este pipeline est√° basado en eventos: todo ocurre autom√°ticamente en cuanto un archivo llega al Data Lake.

![Arquitectura](arquitectura-etl-pipeline.png)

## üîÑ Flujo Completo del ETL

1.  **Ingesta:** Un archivo JSON es cargado en la carpeta de entrada del bucket S3: `orders-json-incoming`.
2.  **Activaci√≥n Autom√°tica:** S3 detecta el nuevo archivo y ejecuta una funci√≥n AWS Lambda mediante un trigger nativo.
3.  **Transformaci√≥n:** La Lambda procesa el archivo utilizando Python + Pandas:
    * Lee el JSON.
    * Aplana la estructura.
    * Convierte los datos al formato Parquet optimizado para an√°lisis.
4.  **Almacenamiento Optimizado:** El archivo procesado se mueve a la carpeta de salida en el Data Lake: `orders_parquet_datalake`.
5.  **Catalogaci√≥n:** La Lambda invoca autom√°ticamente un crawler de AWS Glue para actualizar las tablas del Data Catalog.
6.  **An√°lisis:** En minutos, los datos est√°n listos para ser consultados en Amazon Athena usando SQL est√°ndar.

## üõ†Ô∏è Tecnolog√≠as Utilizadas

| Servicio | Rol en la soluci√≥n |
| :--- | :--- |
| **AWS CloudFormation** | IaC para definir y desplegar toda la arquitectura. |
| **Amazon S3** | Data Lake (almacenamiento de crudos + procesados). |
| **AWS Lambda** | Motor de transformaci√≥n ETL (Python 3.12 + Pandas). |
| **AWS Glue Crawler** | Catalogaci√≥n autom√°tica de metadatos. |
| **Amazon Athena** | Consultas SQL interactivas sobre archivos Parquet. |
| **AWS IAM** | Seguridad y control de permisos (m√≠nimo privilegio). |

## üöÄ Despliegue del Proyecto en AWS

Antes de desplegar aseg√∫rate de tener:
* ‚úî AWS CLI instalada.
* ‚úî Ejecutado `aws configure`.
* ‚úî Permisos para crear recursos (S3, Lambda, Glue, IAM, CloudFormation).

### 1Ô∏è‚É£ Preparar el entorno
Abre tu terminal y navega a la carpeta del proyecto:
```bash
cd "/c/proyectos-aws/project-03-etl-pipeline"
```

### 2Ô∏è‚É£ Crear el bucket para artefactos de CloudFormation
Necesitamos un bucket temporal para subir el c√≥digo de la Lambda antes del despliegue.
```bash
aws s3 mb s3://artifacts-mi-etl-pipeline-565393068619
```

### 3Ô∏è‚É£ Empaquetar la Infraestructura
Este comando sube el c√≥digo local de la Lambda al bucket de artefactos y prepara la plantilla para el despliegue.
```bash
aws cloudformation package \
  --template-file template.yaml \
  --s3-bucket artifacts-mi-etl-pipeline-565393068619 \
  --output-template-file packaged.yaml
```

### 4Ô∏è‚É£ Desplegar el Stack (Deploy)
Este comando lee el archivo empaquetado y construye todos los recursos en tu cuenta de AWS.
```bash
aws cloudformation deploy \
  --template-file packaged.yaml \
  --stack-name mi-etl-pipeline-stack \
  --capabilities CAPABILITY_NAMED_IAM
```

## üìä Verificaci√≥n y An√°lisis
Para verificar que todo funcion√≥:
1. Sube cualquier archivo JSON de prueba a la carpeta: orders-json-incoming en tu bucket S3.
2. Espera unos segundos y revisa la carpeta: orders_parquet_datalake.
3. All√≠ aparecer√° el archivo Parquet generado autom√°ticamente.
4. Verifica que el Glue Crawler haya actualizado el cat√°logo.
5. Ve a Amazon Athena: Selecciona la base de datos etl_database y ejecuta:
```bash
SELECT * FROM "etl_database"."orders_parquet_datalake" limit 10;
```

