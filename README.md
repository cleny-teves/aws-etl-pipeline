# ğŸš€ AWS Serverless ETL Pipeline (IaC con CloudFormation)

Este repositorio contiene una soluciÃ³n 100% Serverless para construir una canalizaciÃ³n ETL totalmente automatizada sobre AWS.

Toda la infraestructura es desplegada utilizando **Infraestructura como CÃ³digo (IaC)** con **AWS CloudFormation**, lo que permite reproducibilidad, escalabilidad y cero configuraciÃ³n manual.

El objetivo es transformar datos crudos de ventas (JSON) en un formato optimizado **Parquet**, almacenarlos en un Data Lake y dejarlos listos para ser consultados con SQL mediante **Amazon Athena**.

## ğŸ›ï¸ Arquitectura de la SoluciÃ³n

Este pipeline estÃ¡ basado en eventos: todo ocurre automÃ¡ticamente en cuanto un archivo llega al Data Lake.

![Arquitectura](arquitectura-etl-pipeline.png)

## ğŸ”„ Flujo Completo del ETL

1.  **Ingesta:** Un archivo JSON es cargado en la carpeta de entrada del bucket S3: `orders-json-incoming`.
2.  **ActivaciÃ³n AutomÃ¡tica:** S3 detecta el nuevo archivo y ejecuta una funciÃ³n AWS Lambda mediante un trigger nativo.
3.  **TransformaciÃ³n:** La Lambda procesa el archivo utilizando Python + Pandas:
    * Lee el JSON.
    * Aplana la estructura.
    * Convierte los datos al formato Parquet optimizado para anÃ¡lisis.
4.  **Almacenamiento Optimizado:** El archivo procesado se mueve a la carpeta de salida en el Data Lake: `orders_parquet_datalake`.
5.  **CatalogaciÃ³n:** La Lambda invoca automÃ¡ticamente un crawler de AWS Glue para actualizar las tablas del Data Catalog.
6.  **AnÃ¡lisis:** En minutos, los datos estÃ¡n listos para ser consultados en Amazon Athena usando SQL estÃ¡ndar.

## ğŸ› ï¸ TecnologÃ­as Utilizadas

| Servicio | Rol en la soluciÃ³n |
| :--- | :--- |
| **AWS CloudFormation** | IaC para definir y desplegar toda la arquitectura. |
| **Amazon S3** | Data Lake (almacenamiento de crudos + procesados). |
| **AWS Lambda** | Motor de transformaciÃ³n ETL (Python 3.12 + Pandas). |
| **AWS Glue Crawler** | CatalogaciÃ³n automÃ¡tica de metadatos. |
| **Amazon Athena** | Consultas SQL interactivas sobre archivos Parquet. |
| **AWS IAM** | Seguridad y control de permisos (mÃ­nimo privilegio). |

## ğŸš€ Despliegue del Proyecto en AWS

Antes de desplegar asegÃºrate de tener:
* âœ” AWS CLI instalada.
* âœ” Ejecutado `aws configure`.
* âœ” Permisos para crear recursos (S3, Lambda, Glue, IAM, CloudFormation).

### 1ï¸âƒ£ Preparar el entorno
Abre tu terminal y navega a la carpeta del proyecto:
```bash
cd "/c/proyectos-aws/project-03-etl-pipeline"
```

### 2ï¸âƒ£ Crear el bucket para artefactos de CloudFormation
Necesitamos un bucket temporal para subir el cÃ³digo de la Lambda antes del despliegue.
```bash
aws s3 mb s3://artifacts-mi-etl-pipeline-565393068619
```

### 3ï¸âƒ£ Empaquetar la plantilla
Este comando sube el cÃ³digo local de la Lambda al bucket de artefactos y prepara la plantilla para el despliegue.
```bash
aws cloudformation package \
  --template-file template.yaml \
  --s3-bucket artifacts-mi-etl-pipeline-565393068619 \
  --output-template-file packaged.yaml
```

### 4ï¸âƒ£ Desplegar la infraestructura completa
Este comando lee el archivo empaquetado y construye todos los recursos en tu cuenta de AWS.
```bash
aws cloudformation deploy \
  --template-file packaged.yaml \
  --stack-name mi-etl-pipeline-stack \
  --capabilities CAPABILITY_NAMED_IAM
```

### ğŸ“ Estructura del Repositorio 
aws-etl-pipeline/
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ lambda-function.py        # LÃ³gica ETL (JSON â†’ Parquet)
â”‚
â”œâ”€â”€ template.yaml                 # Plantilla IaC original (CloudFormation)
â”œâ”€â”€ packaged.yaml                 # VersiÃ³n empaquetada (generada por AWS CLI)
â”‚
â”œâ”€â”€ orders_etl.json               # Archivo JSON de ejemplo para pruebas
â”œâ”€â”€ README.md                     # DocumentaciÃ³n del proyecto
â””â”€â”€ .gitignore                    # Archivos ignorados por Git

### ğŸ§ª Â¿CÃ³mo probar el pipeline?
Una vez desplegado:

1. Sube cualquier archivo JSON de prueba a la carpeta: orders-json-incoming en tu bucket S3.
2. Espera unos segundos y revisa la carpeta: orders_parquet_datalake.
3. AllÃ­ aparecerÃ¡ el archivo Parquet generado automÃ¡ticamente.
4. Verifica que el Glue Crawler haya actualizado el catÃ¡logo.
5. Abre Amazon Athena y consulta tu tabla con SQL:
```bash
SELECT * FROM orders_parquet LIMIT 10;
```

